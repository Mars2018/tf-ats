## DATA PARAMS ##################################
item_id             = 70088 # 
rand_seed           = 251967169
min_cut             = 0.1

chkpt_dir           = chkpt/mod
#data_dir            = /home/david/data/ats/ets
data_dir            = /home/david/data/ets1b/2016
vocab_file          = vocab_n250.txt

## TRAINING PARAMS ##############################
batch_size          = 32
optimizer           = adam # adam rmsprop
learning_rate       = 0.001
epochs              = 50
max_grad_norm       = 2.0
dropout             = 0.5
valid_cut           = 0.2
print_every         = 10
# learning_rate_decay = 0.75

## EMBEDDING ####################################
embed_type          = char

## char embedding -------------------------------
kernel_widths       = '[1,2,3,4,5,6,7]'
char_embed_size     = 15
kernel_features     = '[25,50,75,100,100,100,100]'
char_embed_chkpt    = /home/david/code/python/tf-lstm-char-cnn/chkpt/mod2_600-15

#char_embed_size     = 20
#kernel_features     = '[50,100,150,200,200,200,200]'
#char_embed_chkpt    = /home/david/code/python/tf-lstm-char-cnn/chkpt/mod1_650-20

## word embedding -------------------------------
embed_path          = /home/david/data/embed/glove.6B.{}d.txt
embed_dim           = 300
min_word_count      = 10

## MODEL PARAMS #################################
rnn_unit            = lstm # lstm gru rwa rhn
rnn_dim             = 300
rnn_layers          = 1
# max_text_length     = 1000

trim_words
# bidirectional
# train_initial_state
# peepholes
# skip_connections

## RNN AGGREGATION ##############################
mean_pool
att_size            = 100

# num_highway_layers  = 1
